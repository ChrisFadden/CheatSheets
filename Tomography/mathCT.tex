\documentclass[14pt]{extarticle}
\usepackage{researchPaper}
\usepackage{outlines}
\usepackage{mathrsfs}
\def\Definition{{\color{blue} \textbf{Definition:} }}
\def\Theorem{{\color{red} \textbf{Theorem:} }}
\newcommand*\pFq[2]{{}_{#1}F_{#2}}

\title{Mathematics of Computerized Tomography Cheat Sheet}
\begin{document}
	\maketitle	
	\begin{outline}		
		\section*{Fourier Transform}
			\1	Continuous Functions $C^{\infty}(\mathbb{R})$
				\2	Rigorous Definition of Continuous Function $C^0(\mathbb{R})$
					\3	$f~:~\mathbb{R} \rightarrow \mathbb{R}$
					\3	$f~:~x \mapsto y$
					\3	Open set $(y_0,y_1)$ in $y$ has a preimage $(x_0,x_1) \equiv f^{-1}(y_0,y_1)$
							that is also open
					\3	\textbf{EXAMPLE:} What about $f(x) = x^2?$
						\4	$(-2,2) \mapsto [0,4)$?
						\4	Answer:	The initial open set must be in the image, i.e in $y$,
								and those are definitely open in $x$	
				\2	$C^k$ Functions
					\3	$C^1$ Functions are functions with a continuous first derivative
					\3	$C^k$ Functions are functions with $k$ continuous derivatives
					\3	\textbf{EXAMPLE:}
						\4	$f(x) = \begin{cases} x & x \ge 0 \\ 0 & x < 0   \end{cases}$
						\4	Is Continuous, but not differentiable, so $C^0(\mathbb{R})$
					\3	\textbf{EXAMPLE:}
						\4	$f(x) = x^2sin(\frac{1}{x})$, $f(0) = 0$
						\4	$f'(x) = -cos(\frac{1}{x}) + 2x sin(\frac{1}{x})$, $f'(x) = 0$
						\4	This function is differentiable, but the derivative is not 
								continuous.  
						\4	$C^1$ is a stronger condition than just being differentiable
					\3	\textbf{EXAMPLE:}
						\4	$f(x) = \abs{x}^{k+1}$
						\4	Has $k$ continuous derivatives, so $C^{k}$ (if $k$ is even)
				\2	$C^{\infty}$ Functions
					\3	Functions with an infinite amount of continuous derivatives 
							($C^{\infty}$) are called "smooth"
					\3	\textbf{EXAMPLE:}
						\4	$f(x) = e^{-1/x}$, $f(x<0) = 0$
						\4	Has infinitely many continuous derivatives
				\2	Analytic Function $C^{\omega}$
					\3	Analytic functions are functions that have a convergent Taylor
							Series
					\3	Analytic Functions are always smooth ($C^{\infty}$)
					\3	If a complex function has a convergent complex Taylor (Laurent)
							Series, then it is complex differentiable (a very nice property)
					\3	\textbf{EXAMPLE:}
						\4	Polynomials
						\4	Exponential functions
						\4	Trig functions, logarithm functions
					\3	\textbf{EXAMPLE:}
						\4	The function $f(x) = e^{-1/x}$, $f(x<0) = 0$, while smooth
								is not analytic
						\4	ALL ANALYTIC FUNCTIONS ARE SMOOTH
						\4	NOT ALL SMOOTH FUNCTIONS ARE ANALYTIC	
			\1	Lesbegue Space ($L_p$)
				\2 $L_p(\mathbb{R}) \equiv \forall~f~:~(\int_S \abs{f}^p d\mu)^{\frac{1}{p}} < \infty$
					\3	$S$ is a set
					\3	$\mu$ is a measure on the set $S$ and a $\sigma$-algebra
					\3	Measure theory is a whole course by itself, and is needed for the
							completely rigorous definition...
				\2	$L_2$ Spaces
					\3	$f~:~\sqrt{\int_{-\infty}^{\infty} \abs{f(x)}^2 dx} < \infty$
					\3	Square-integrable functions
					\3	Very popular function space, since it forms a natural inner
							product, a desirable property when doing math/physics
				\2	$L_1$ Spaces
					\3	$f~:~\int_{-\infty}^{\infty} \abs{f(x)} dx < \infty$
					\3	Absolutely integrable functions
					\3	Less nice than $L_2$ (no natural inner product)
					\3	Functions can be in $L_1$, but not $L_2$
					\3	\textbf{EXAMPLE:}
						\4	$f(x) = \frac{1}{\sqrt{x}}$, $x = (0,1)$
						\4	As $x \rightarrow 0$, $L_1$ has no problem, but $L_2$ transforms
								it into $log(x)$, which has a problem as $x \rightarrow 0$
					\3	\textbf{EXAMPLE:}
					\3	Functions can be in $L_2$, but not in $L_1$
						\4	$f(x) = \frac{1}{x}$ (Harmonic series integral doesn't converge)
			\1	Fourier Transform
				\2	Fourier Transform: $\hat{f}(k) = (2\pi)^{-n/2} \int_{\mathbb{R}^n}e^{-jxk}f(x) dx$
				\2	Inverse Fourier Transform: $\bar{f}(x) = (2\pi)^{-n/2} \int_{\mathbb{R}^n}e^{jxk} \hat{f}(k) dk$
				\2	Transform formula are valid only for functions in $L_1$
				\2	Defining the transform this way, with the $(2\pi)^{-n/2}$ out front
						means that the transform is orthonormal (Very nice property)
				\2	\textbf{EXAMPLE: 1D}
					\3	$\hat{f}(k) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-jxk}f(x) dx$
					\3	$\bar{f}(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{jxk} \hat{f}(k) dk$
			\1	Fourier Properties ($L_1(\mathbb{R}^n)$)
				\2	R1 (Scaling Property):	$f(cx) \mapsto \frac{1}{\abs{c}^n} \hat{f}(\frac{k}{c})$
				\2	R2 (Shift Property): $f(x - y) \mapsto e^{-jky}\hat{f}(k)$
				\2	R3 (Derivative Property): $\pd[m]{f}{x} \mapsto j^{\abs{m}}k^m \hat{f}(k)$
					\3	(Holds for $m < 0$, i.e. integrals instead of derivatives)
				\2	R3	(Derivative in Frequency): $(x^mf(x)) \mapsto j^{\abs{m}} \pd[m]{f}{k}$
				\2	R4 (Convolution):	$f * g \mapsto (2\pi)^{n/2} \hat{f}\hat{g}$
				\2	R4 (Convolution):	$fg \mapsto (2\pi)^{-n/2}\hat{f} * \hat{g}$
				\2	R5 (Parseval Relation): $\int_{\mathbb{R}^n} f \hat{g} dx = \int_{\mathbb{R}^n}\hat{f}gdx$
				\2	Plancharel's Theorem:	
					\3	If $f$ is in both $L_1$ and $L_2$ then:
					\3	$\int_{-\infty}^{\infty} \abs{f(x)}^2 dx = \int_{-\infty}^{\infty} \abs{\hat{f}(k)}^2 dk$
			\1	Compact Support
				\2	Functions with compact support in general are only nonzero over a compact subset
				\2	In Euclidean space $\mathbb{R}^n$, this means than it is nonzero
						over a closed ($[a,b]$) subset that is bounded
				\2	Fourier Transform a compactly supported function is complex analytic $C^{\omega}(\mathbb{C}^n)$
			\1	Schwartz Space
				\2	$\mathscr{S}(\mathbb{R}^n)$ contains functions that are smooth
						($C^{\infty}$) and for which the supremeum ($sup~\approx~max$)
						of $\abs{x^a \pd[b]{f}{x}} < \infty$ for all $a,b \in \mathbb{N}^n$
				\2	Schwartz functions decay rapidly (faster than any polynomial)
				\2	Functions with compact support are in $\mathscr{S}$
				\2 Schwartz Spaces are where $\bar{\hat{f}} = \hat{\bar{f}} = f$
					\3	i.e. The Fourier transform can be inverted to recover the original function
					\3	Fourier inversion can technically be done a slightly larger class
							of functions in $L_1$, but certainly not all of $L_1$, and definitely
							not all of $L_2$	
			\1	Dual Space 
				\2	Linear Algebra Analog to Functional Analysis
					\3	What we've done so far (a crash course in functional analysis) has
							an analog in finite dimensional linear algebra
					\3	In linear algebra, we have  $x = [x_1,x_2,...,x_n]^T$ as
							a column vector
					\3	In what we have done, the functions are equivalent to the elements
							in the column vector
				\2	Definition of Dual Space
					\3	A dual space is different from a function space (column vector).
					\3	A function over the real numbers takes a number $x \in \mathbb{R}$
							as input, and returns $y \in \mathbb{R}$ as the image
					\3	A real linear functional takes as input a function $x \in C^k(\mathbb{R})$
							and returns a real number $y \in \mathbb{R}$
					\3	Linear Functionals, members of the dual space, can be thought of
							as the "transpose" of the function, as a row vector
					\3	Linear functional $\approx$ $x^T = [x_1,x_2,...,x_n]$
				\2	Functionals are only rigorously defined by how they
						act on other functions.  Their properties can be found in an abstract
						setting, but any calculation must involve other actual functions
			\1	Tempered Distributions
				\2	Elements of the Dual Space to Schwartz Functions, $T \in \mathscr{S}'$
						are called tempered distributions
				\2	Distributions are differentiated according to:
					\3	$\pd[m]{Tf}{x} = (-1)^{\abs{m}} T \pd[m]{f}{x}$
				\2	The Fourier Transform is extended to tempered distributions via:
					\3	$\hat{T}f = T\hat{f}$
			\1	Dirac Delta Function
				\2	The Dirac Delta Function $\delta \in \mathscr{S}'$ is a tempered distribution
				\2	As a tempered distribution, it is defined by how it acts on functions.
						The Dirac delta function could be defined as
						$\delta(f,a) \equiv \delta_af = f(a)$
						\3	$\delta~:~C^k(\mathbb{R}) \cross \mathbb{R} \rightarrow \mathbb{R}$
				\2	This is usually written (through abuse of notation) as:
						$\int_{-\infty}^{\infty} f(x) \delta(x - a) = f(a)$
				\2	Applying the formula for finding the Fourier Transform of a distribution
						leads to $\delta_a(x) \mapsto (2\pi)^{-n/2}e^{-jka}$
				\2	The inverse Fourier transform, finding a $g(k) \mapsto \delta_a(x)$
						is a little more involved, and basically comes down to finding a
						limiting process definition of the delta function
						\3	Different authors have different preferences for defining the
								limiting process.  Natterer has chosen to go with Bessel functions,
								other authors use Gaussians that continuously shrink in width,
								others use sinc functions that continuously shrink in width.
						\3	Several different functions can converge to the delta function,
								and so each is equally valid, and it comes down to personal
								preference how you define it.  Engineers ignore it all together
								and just say the inverse Fourier transform of a complex exponential
								is the delta function
			\1	Hilbert Transform
				\2	A second distribution is the Cauchy principal value integral, which
						is a functional in the dual Schwartz space $(\mathscr{S}')$
				\2	The Cauchy Principal Value Integral is defined as:
					\3	$Tf \equiv \int \frac{f(x)}{x} dx = \int_{-\infty}^{\infty} \frac{f(x) - f(-x)}{2x}dx$
				\2	The Fourier Transform has a derivation that Natterer does, and it
						comes out to:
					\3	$\hat{T}f = -j (\frac{\pi}{2})^{1/2} \int_{-\infty}^{\infty} sgn(k) f(k) dk$
					\3	$\hat{T}(k) = -j (\frac{\pi}{2})^{1/2}sgn(k)$
				\2	The Hilbert Transform is the convolution of the functional and the
						function, with a factor of $\frac{1}{\pi}$, and is again another
						functional $H \in \mathscr{S}'$
					\3	$Hf(x) = \frac{1}{\pi}\int_{-\infty}^{\infty} \frac{f(y)}{x-y}dy$
				\2	The Fourier Transform can again be found using the distributional form as
					\3	$Hf(x) \mapsto -j sgn(k) \hat{f}(k)$
					\3	Therefore, the Hilbert transform of a function adds a phase shift
							to the Fourier transform
				\2	\textbf{EXAMPLE:}
					\3	$H 1 = \frac{1}{\Pi t}$ where $\Pi$ is the "Shah" distribution
						\4	$\Pi$ is just the sum of delta functions, also known as a comb filter
					\3	$H sin(\omega t) = -sgn(\omega) cos(\omega t)$
					\3	$H cos(\omega t) = sgn(\omega) sin(\omega t)$
					\3	$H \frac{1}{t^2 + 1} = \frac{t}{t^2 + 1}$
				\2	Hilbert Transforms can be used to ensure that a signal has no
						negative frequency components
					\3	$a(t) = f(t) + j Hf(t)$
					\3	$a(t)$ is called the analytic representation of $f(t)$
					\3	This can be very useful in signal processing applications, 
							especially communications, where you are modulating baseband
							signals and want to efficiently manipulate the modulated signal.
					\3	NOTE: ANALYTIC REPRESENTATION DOES NOT MEAN ITS ANALYTIC (nice properties
							including complex differentiation etc.)
		\section*{Special Functions}
			\1	Best Book is Abramowitz and Stegun Handbook of Mathematical Functions
			\1	NIST Handbook of Mathematical Functions pdf can be found online
			\1	Gamma Function
				\2	Definition
					\3	$\Gamma(z) \equiv \int_0^{\infty} x^{z-1} e^{-x} dx$
					\3	$\Gamma(z) = \frac{e^{-\gamma z}}{z}\Pi_{n=1}^{\infty} (1 + \frac{z}{n})^{-1} e^{z/n}$
						\4	$\gamma \approx 0.577$ is the Euler-Mascheroni constant
				\2	Properties
					\3	$\Gamma(n) = (n-1)!$
					\3	$z \Gamma(z) = \Gamma(z+1)$	
					\3	$\Gamma(1-z)\Gamma(z) = \frac{\pi}{sin(\pi z)}$
					\3	$\Gamma(z)\Gamma(z+\frac{1}{2}) = 2^{1 - 2z} \sqrt{\pi} \Gamma(2z)$
					\3	$\Gamma(\frac{1}{2}) = \sqrt{\pi}$
					\3	Complex Analytic (Very nice) over $\mathbb{C}$ except the negative integers...
				\2	Derivative of Gamma Function
					\3	$\Gamma'(m+1) = m!(-\gamma + \sum_{k=1}^m \frac{1}{k})$
					\3	$\od[m]{\Gamma(x)}{x} = \int_0^{\infty} t^{x-1}e^{-t}(\text{ln} t)^n dt$
			\1	Bessel Function
				\2	Solution of Bessel's differential equation
					\3	$x^2 \od[2]{y}{x} + x \od{x}{y} + (x^2 - \alpha^2)y = 0$
					\3	Comes up alot when dealing with Cylindrical and Spherical Coordinate systems
					\3	Especially calculating Laplacians
				\2	Definition (First Kind)
					\3	$J_{\alpha} = \sum_{m=0}^{\infty} \frac{(-1)^m}{m! \Gamma(m + \alpha + 1)} (\frac{x}{2})^{2m+\alpha}$	
					\3	$J_{\frac{-1}{2}} = \sqrt{\frac{2}{\pi x}} cos(x)$
					\3	$J_{\frac{1}{2}} = \sqrt{\frac{2}{\pi x}} sin(x)$
					\3	$J_{n} = \sum_{m=0}^{\infty} \frac{(-1)^m}{2^{2m + n} m! (n + m)!}x^{2m + n}$
						\4	The follow only valid for integer $n$
					\3	$J_{-n}(x) = (-1)^nJ_n(x)$
					\3	$J_n(x) = \frac{1}{\pi}\int_0^{\pi} \cos(n \tau - x \sin(\tau))d\tau$
					\3	$J_n(x) = \frac{1}{2 \pi} \int_{-\pi}^{\pi} e^{jx \sin(\theta) - j n \theta}d\theta$
					\3	Bessel functions of any order are orthogonal (very nice property)
						\4	Can be written like a Fourier Series
						\4	$f(r,\theta) = \sum_{n=-\infty}^{\infty} \sum_{m=1}^{\infty}
									f_{nm}e^{jn\theta} J_n(\frac{ra_m}{a})$
						\4	$a_m$ are the zeros of the zeros of the Bessel function of order $m$
						\4	$a$ is a slightly more complicated costant... (makes the wronskian
								zero at $r = a$ for orthogonality...)
				\2	Definition (Second Kind)
					\3	$Y_{\alpha} = \frac{J_{\alpha}(x)\cos(a\pi) - J_{-\alpha}(x)}{\sin(a\pi)}$
						\4	Not valid for integer $\alpha$
					\3	$Y_{\alpha}(0) \rightarrow -\infty$
						\4	Has a singularity at the origin
					\3	Also a solution to Bessel's equation
						\4	In same way that sines and cosines are solutions to harmonic
								oscillator, $J_{\alpha}(x)$ and $Y_{\alpha}$ are solutions
								to Bessel's differential equation
				\2	Hankel Functions
					\3	$H_{\alpha}^{(1)} = J_{\alpha}(x) + j Y_{\alpha}(x)$ (First Kind)
					\3	$H_{\alpha}^{(2)} = J_{\alpha}(x) - j Y_{\alpha}(x)$ (Second Kind)
					\3	Also solutions to Bessel's differential equation (linear combination of solutions)
					\3	Form Green's Functions for Helmholtz Equation in 2D
						\4	$G(\bm{x};\bm{y}) = \frac{j}{4}H_0^{(1)}(k_0\rho)$, $\rho = \abs{\bm{x} - \bm{y}}$
						\4	$G(\bm{x};\bm{y}) = \frac{-j}{4}H_0^{(2)}(k_0\rho)$
						\4	Choice depends on what direction the wave is propogating
			\1	Gegenbauer Polynomials
				\2	Special Polynomials that are orthogonal on $[-1,1]$ (Which can be
						shifted to any interval you want
				\2	Orthogonal with respect to the weight $(1 - x^2)^{\lambda - 1/2}$
				\2	$\int_{-1}^1 (1-x^2)^{\lambda - 1/2}C_l^{\lambda}(x)
						C^{\lambda}_k(x) dx$
					\3	$= 0$ if $l \ne k$
					\3	$= \frac{2^{2\lambda -1}(\Gamma(\lambda + \frac{1}{2}))^2l!}{(l+\lambda)\Gamma(l + 2\lambda)}$
							for $l = k$
				\2	Chebyshev Polynomials ($\lambda = 0$)
					\3	$C_l^0(x) = T_l(x) = \cos(l \cos^{-1}(x))$ $\abs{x} \le 1$
					\3	$T_l(x) = \cosh(l \cosh^{-1}(x))$ $\abs{x} \ge 1$
					\3	Solutions to Chebyshev's differential equation
					\3	Best polynomial approximation in $L_{\infty}$ norm to an arbitrary function
				\2	Legendre Polynomials ($\lambda = \frac{1}{2}$)
					\3	$C_l^{1/2} = P_l = \frac{1}{2^ll!}\od[l]{}{x}(x^2 - 1)^l$
						\4	Can be generalized to Legendre functions by having arbitrary
								(non-integer) order $l$ and using $\Gamma$ functions
					\3	Solutions to Legendre's differential equation
					\3	Best polynonomial approximation in $L_2$ norm to an arbitrary function
					\3	Associated Legendre Polynomials
						\4	$P_l^m = (-1)^m (1-x^2)^{m/2}\od[m]{}{x}(P_l(x))$
				\2	Fourier Transform of Gegenbauer Polynomials
					\3	$\hat{(wC_m^{\lambda})}(\omega) = \frac{\Gamma(2\lambda)}{\Gamma(\lambda)}
								\sqrt{2\pi}2^{-\lambda} j^{-m}\omega^{-\lambda}J_{m+\lambda}(\omega)$
					\3	Gegenbauer polynomials and Bessel Functions are intimately connected
					\3	Gegenbauer polynomials can be even further generalized by 
							Jacobi polynomials
			\1	Spherical Harmonics
				\2	Spherical harmonics are solutions to Laplace's equation
						$\nabla^2 f = 0$ in spherical coordiantes
				\2	The solutions are $Y_l^m(\theta,\phi) = N e^{j m \phi} P_l^m(cos(\theta))$
			\1	Hypergeometric Function
				\2	$\pFq{2}{1}(a,b;c;z) = \frac{\Gamma(c)}{\Gamma(a)\Gamma(b)}
							\sum_{n=0}^{\infty} \frac{\Gamma(a + n)\Gamma(b + n)}{\Gamma(c + n)\Gamma(1+n)}
							z^n$
				\2	$\pFq{m}{n}$ has a generalization, where it has $m$ first arguments
						before the $;$, and $n$ second arguments before the second $;$, with
						the associated Gamma Functions.  The last argument is always the
						argument to the function ($z$ or $x$).
				\2	$C_n^{\lambda} = \binom{n+2\lambda - 1}{n}	
							\pFq{2}{1}(-n,n+2\lambda;\lambda + \frac{1}{2};\frac{1}{2}(1-x))$
				\2	$J_{\alpha}(x) = \frac{(x/2)^{\alpha}}{\Gamma(\alpha + 1)}
							\pFq{0}{1}(\alpha + 1; - \frac{x^2}{4})$
				\2	Hypergeometric Functions generalize nearly every function you've
						ever heard of, and studying their properties can give insight into
						their behavior
		\section*{Sobolev Spaces}
			\1	Cauchy Sequences
				\2	A sequence $x_1,x_2,... \in X$
				\2	$\forall \varepsilon > 0$, $\exists~N \in \mathbb{N}$ such that for
						$m,n \in \mathbb{N}$, $m,n > N$, $d(x_m,x_n) < \varepsilon$
				\2	In $\mathbb{R}$, $\abs{x_m - x_n} < \varepsilon$
				\2	Every Cauchy sequence is bounded
				\2	Every convergent sequence is Cauchy
				\2	NOT every Cauchy sequence converges in the set
				\2	Completeness
					\3	A property of a metric space where every Cauchy sequence converges
							to an element of $X$
					\3	EXAMPLE:	$\mathbb{R}$, $\mathbb{R}^n$, $\mathbb{C}$
					\3	EXAMPLE (NOT): $\mathbb{Q}$ is not complete (a sequence of rationals
							could converge to $\pi$)
			\1	Banach Space
				\2	Operator Norm
					\3	$T~:~X \rightarrow Y$ Linear Map
					\3	$\norm{T} = \sup\{\norm{Tx}_Y~:~x \in X, \norm{x}_X \le 1\}$
					\3	EXAMPLE: $T~:~L_1(\mathbb{R}) \rightarrow L_2(\mathbb{R})$
						\4	$\norm{T} = \sup\{\sqrt{\int \abs{T[f(x)]}^2 dx}~:~f(x) \in L_1(x),
									\int \abs{f(x)} dx < 1$
				\2	Definition
					\3	A Banach Space of functions is a complete vector space with a norm (the operator norm)
					\3	Vector space has the same axiomatic properties that you learned in Linear algebra
							(though can have infinite basis elements)
					\3	Complete means every Cauchy sequence converges
			\1	Hilbert Space
				\2	Definition
					\3	A Hilbert Space is a Banach Space where the norm induces an 
							inner product (dot product)
					\3	Norm will induce an inner product iff:
						\4	$\forall~x,y \in X~:~\norm{x+y}^2 + \norm{x-y}^2 = 2(\norm{x}^2+\norm{y}^2)$
						\4	Shows that $L_2(\mathbb{R})$ is the only Lesbegue space with an
								inner product
						\4	$L_2(\mathbb{R})$ is a Hilbert space with inner product
								$\braket{f}{g} = \int f(x)g(x) dx$
				\2	Adjoint Operator
					\3	Adjoint Operator is roughly the hermitian transpose of a function
							in a Hilbert Space (The adjoint shares most properties of Hermitian transpose)
					\3	$A~:~H_1 \rightarrow H_2$ is the standard (forward) operator between Hilbert Spaces
					\3	$A^*~:~H_2 \rightarrow H_2$ is the adjoint operator
					\3	$\braket{Ah_1}{h_2}_{H_2} = \braket{h_1}{A^*h_2}_{H_1}$
					\3	In the operator norm, $\norm{A^*A} = \norm{A}^2$
				\2	Self-adjoint Operator
					\3	An operator that is equal to its adjoint
					\3	Generalization of "Symmetric" or "Hermitian" matrices
					\3	$\braket{Ah_1}{h_2} = \braket{h_1}{Ah_2}$
				\2	Spectral Theorem
					\3	Normal Operator
						\4	A normal operator commutes with its hermitian adjoint
								$A^*A = AA^*$
						\4	More general than self-adjoint
						\4	Unitary operator $U^*U = UU^* = I$ (similar to Orthogonal matrices)
					\3	The Spectral theorem holds for normal operators on a Hilbert Space
							(including self-adjoint operators)
					\3	Spectral theorem states that a normal operator on a Hilbert Space
							can be reduced to a multiplication operator in a suitable basis
					\3	EXAMPLE:
						\4	Let $f$ be a function, $U~:~H \rightarrow L_2(\mathbb{R})$ be a unitary operator,
								and $A$ be a normal operator
						\4	There exists $\exists~[T_fg](x) = f(x)g(x)$ a multiplication operator 
								(generalization of a diagonal matrix)
						\4	$\norm{T} = \norm{f}_{\infty}$ where $L_{\infty}$ norm is the
								maximum value, $L_{\infty}[f(x)] = \sup\{f(x)\}$
						\4	$U^* T U = A$
					\3	EXAMPLE:
						\4	Derivative with Fourier Transform
						\4	$U \equiv \mathcal{F}$ The unitary operator is the Fourier Transform
						\4	$[T_fg](\omega) = j\omega g(\omega)$
						\4	$A = \pd{}{t}$
						\4	$\mathcal{F}^{-1} j \omega \mathcal{F} = \pd{}{t}$
					\3	Spectral Theorem is what allows a normal matrix to have an
							eigendecomposition with orthonormal eigenvectors
					\3	Spectral Theorem allows the application of things like eigenvalues
							and eigenvectors to more general function spaces rather than
							just matrices (extremely powerful)
			\1	Sobolev Space
					\2	Definition
						\3	$W^{m,p}(\mathbb{R}) = \{u \in L_p(\mathbb{R}),~
								 \partial^{\alpha} u \in L^p(\mathbb{R}),~\forall~\alpha \in \mathbb{N}^d,
									\abs{\alpha} \le m\}$
						\3	$\norm{u}_{W^{m,p}} = ( \sum_{\abs{\alpha \le m}} \int \abs{\partial^{\alpha}u}^pdx)^{1/p}$
						\3	Sobolev spaces $W^{m,p}(\mathbb{R})$ consist of 
								functions in $L_p(\mathbb{R})$ with up to the $m$th derivative
								also in $L^p(\mathbb{R})$
						\3	All Sobolev spaces are Banach spaces
						\3	A useful notation specializes to $L_2$ spaces, 
								$W^{m,2}(\mathbb{R}) = H^m(\mathbb{R})$, which is a Hilbert Space
					\2	EXAMPLE:
						\3	$H^1(\mathbb{R}) = u \in L_2(\mathbb{R})$ such that
									$\nabla u \in L_2(\mathbb{R})$
						\3	$\norm{u} = (\int \abs{u(x)}^2 dx + \int \abs{\nabla u(x)}^2 dx)^{1/2}$
					\2	Sobolev Spaces are generally the function spaces considered when
							solving partial differential equations, because of the Trace Theorem
					\2	Trace Theorem
						\3	Definition
							\4	Assume $\Omega$ is a bounded domain, with Lipschitz (nice, continuous) boundary $\partial \Omega$
							\4	$T~:~W^{1,p}(\Omega) \rightarrow L^p(\partial \Omega)$
							\4	$Tu = u|_{\partial \Omega}$ for $u \in W^{1,p}(\Omega) \cap C(\bar{\Omega})$
							\4	$\norm{Tu}_{L_p(\partial \Omega)} \le c(p,\Omega) \norm{u}_{W^{1,p}}(\Omega)$ for 
									$u \in W^{1,p}(\Omega)$
						\3	$Tu$ is called the trace of $u$
						\3	It says that the trace is equal to the function on the boundary,
								if the function is in a suitable function space
						\3	In just the Sobolev space, the trace is bounded by a constant
								times the function
						\3	Extremely useful when finding function spaces for boundary conditions
					\2	Natterer Definition of Sobolev Space
						\3	$H^{\alpha} = \{f \in \mathscr{S}'(\mathbb{R}^n)~:~
									(1 + \abs{\omega}^2)^{\alpha/2} \hat{f} \in L_2(\mathbb{R}^n)$
						\3	Functions in the dual Schwartz space whose weighted Fourier 
								transforms are in $L_2$
						\3	The definition is fairly equivalent, over only the domain of
								his definition (i.e. Tempered distributions)
	\end{outline}
\end{document}

