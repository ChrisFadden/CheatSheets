\documentclass[14pt]{extarticle}
\usepackage{researchPaper}
\usepackage{outlines}

\title{Statistical Signal Processing Cheat Sheet}
\begin{document}
	\maketitle
	
	%Statistical Signal Processing Vol 1:  Estimation   Stephen Kay
	
	\begin{outline}		
		\1	Minimum Variance Unbiased Estimation	%Chap 2
			\2	Unbiased Estimators
				\3	$E(\hat{\theta}) = \theta$
				\3	$\theta = (a,b)$
				\3	$E(\hat{\theta}) = \int g(\bm{x}) p(\bm{x} | \hat{\theta}) d\bm{x} = \theta~\forall~\theta$
			\2	Bias of an Estimator
				\3	$b(\theta) = E(\hat{\theta}) - \theta$
			\2	Minimum Variance Criterion
				\3	Mean Squared Error
					\4	$MSE(\hat{\theta}) = E[(\theta - \hat{\theta})^2]$
					\4	$MSE(\hat{\theta}) = var(\hat{\theta}) + b(\theta)^2$
				\3	If a bias exists, since it depends on the parameter, the estimator
						is not very useful.  Therefore, it makes sense to find an unbiased
						esimator, which obtains its optimal value by minimizing the variance
						of the estimated parameter
				\3	Unbiased estimator may or may not exist
			\2	Finding the Minimum Variance Unbiased (MVU) Estimator
				\3	Determine a Cramer Rao Lower Bound (CRLB), and find an estimator that satisfies it
					\4	An estimator who achieves a variance equal to the CRLB $\forall~\theta$ must be a MVU estimator
				\3	Apply the Rao-Blackwell-Lehmann-Scheffe Theorem
					\4	Find a sufficient statistic which uses all the data
					\4	Find a function of that statistic which is an unbiased estimator of $\theta$
				\3	Restrict estimator to be a linear unbiased estimator
					\4	Specific to particular data sets
		\1	Cramer-Rao Lower Bound	%Chap 3
			\2	Likelihood Functions
				\3	Probability Density Functions that are functions of an unkown parameter
				\3	"Sharpness" of likelihood determines how well the unknown parameter can be estimated
				\3	Sharpness depends on the curvature of the log-likelihood
				\3	Example
					\4	$p_i(x | \mu_0) = \frac{1}{\sqrt{2\pi \sigma_i^2 }} exp(-\frac{1}{2\sigma_i^2}(x - \mu_0)^2)$
					\4	$ln(p_i(x | \mu_0)) = -ln(\sqrt{2\pi \sigma_i^2}) - \frac{1}{2\sigma_i^2}(x - \mu_0)^2$
					\4	$\pd{ln(p_i(x | \mu_0))}{\mu_0} = \frac{1}{\sigma_i^2}(x - \mu_0)$
					\4	$-\pd[2]{ln(p_i(x | \mu_0))}{\mu_0} = \frac{1}{\sigma_i^2}$
				\3	Curvature is $-E(\pd[2]{ln(p_i(x | \mu_0))}{\mu_0})$ to account for 
						when the curvature depends on the parameter
			\2	CRLB
				\3	Regularity Condition
					\4	$E(\pd{ln(p(\bm{x} | \theta))}{\theta} = 0~\forall~\theta$
				\3	Variance Condition
					\4	$var(\hat{\theta}) \ge \frac{1}{-E( \pd[2]{ln(p(\bm{x} | \theta))}{\theta})}$
					\4	$var(\hat{\theta}) \ge \frac{1}{-E( (\pd{ln(p(\bm{x} | \theta))}{\theta})^2)}$	
					\4	Variance of any unbiased estimator must satisfy this expression
					\4	The derivative is taken with respect to the true parameter value
				\3	An unbiased estimator attains the CRLB iff:
					\4	$\pd{ln(p(\bm{x} | \theta))}{\theta} = I(\theta)(g(\bm{x}) - \theta)$
					\4	$\hat{\theta} = g(\bm{x})$
					\4	$min~var = \frac{1}{I(\theta)}$
			\2	Fisher Information
				\3	Fisher Information is $I(\theta) = -E( \pd[2]{ln(p(\bm{x} | \theta))}{\theta})$
				\3	When the CRLB is obtained, the variance is equal to the reciprocal of the 
						Fisher information
				\3	Additional samples carry no more information, the CRLB will not
						decrease with more samples
				\3	Fisher Information Matrix
					\4	Vector valued parameter
					\4	$I(\bm{\theta})_{ij} = -E(\frac{\partial^2ln(\bm{p} | \bm{\theta})}
																								{\partial \theta_i \partial \theta_j})$
			\2	Transformation of parameters with CRLB
				\3	$\alpha = \phi(\theta)$
				\3	$var(\hat{\theta}) \ge \frac{(\pd{\phi}{\theta})^2}{-E( \pd[2]{ln(p(\bm{x} | \theta))}{\theta})}$
				\3	Efficiency of an estimator is lost when using nonlinear transformation
				\3	Affine transformations preserve efficiency
				\3	Nonlinear transformations may asymptotically approach efficient estimators
						in the limit of infinite samples
			
			\2	CRLB with vector parameters
				\3	Regularity Condition
					\4	$E(\pd{ln(p(\bm{x} | \bm{\theta}))}{\bm{\theta}} = 0~\forall~\bm{\theta}$
				\3	Covariance Matrix of parameters $C_{\hat{\theta}}$
				\3	$C_{\hat{\theta}} - I(\bm{\theta})^{-1} \ge \bm{0}$
					\4	Resulting matrix is symmetric postive semi-definite
				\3	$var(\hat{\theta_i}) = C_{\bm{\hat{\theta}}}_{ii} \ge I^{-1}(\bm{\theta})_{ii}$
				\3	Vector Transformations
					\4	$C_{\alpha} - \pd{\bm{\phi}}{\bm{\theta}}I^{-1}(\bm{\theta})
							\frac{\partial \bm{g}(\bm{\theta})^T}{\partial \theta} \ge \bm{0}$

		\1	Maximum Likelihood Estimation	%Chap 7
		\1	Bayesian Estimation %Chap 10 (The Bayesian Philosophy)
	\end{outline}
\end{document}


















