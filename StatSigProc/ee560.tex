\documentclass[14pt]{extarticle}
\usepackage{researchPaper}
\usepackage{outlines}

\title{EE 560 Cheat Sheet}
\begin{document}
	\maketitle
	
	\begin{outline}		
		\1	Basic Definitions
			\2	$P[A] = lim_{n \rightarrow \infty} \frac{n(A)}{n}$
			\2	$P[A] = \frac{N_A}{N}$
			\2	Axioms of Probability
				\3	$P[A] \ge 0$
				\3	$P[\Omega] = 1$
				\3	$P[\bigcup_{i=1}^{\infty} A_i] = \sum_{i=1}^{\infty} P[A_i]$
					\4	$A_i \cap A_j = \emptyset~\forall~i,j~i \ne j$
			\2	Probability Space
				\3	$\Omega$ The Sample Space:	Set of all possible outcomes of an experiment
					\4	Given a set with $N$ elmentary outcomes, there can be $2^N$ events in F
					\4	$2^N$ field of events is called the power set
				\3	$F$ the field of events.  Subsets of sample space which are assigned probabilities
					\4	$\emptyset \in F$
					\4	$\Omega \in F$
					\4	$A \in F$ and $B \in F \rightarrow A\cup B \in F$ and $A \cap B \in F$
					\4	$A \in F \rightarrow \bar{A} \in F$
				\3	$P$ A probability function which assigns a real number to each event in F
		\1	Set Theory
			\2	Set is a collection of objects, with no repetition
			\2	Set membership
				\3	$a \in A$ a is in the set A
				\3	$d \notin A$ d is not in the set A
				\3	$\emptyset = \{\}$ is the empty or null set
			\2	Types of Sets
				\3	Universal set (or Sample Space) is the set of all elements
				\3	Subset
					\4	$A \subset B$ iff $x \in A \rightarrow x \in B$, $x \in B$ does not mean $x \in A$
					\4	$\emptyset \subset A \subset B \subset \Omega$
					\4	$A \subset B$ and $B \subset C \rightarrow A \subset C$
				\3	Equality between sets: $A = B$ iff $A \subset B$ and $B \subset A$
			\2	Set Operations
				\3	Union:	$x \in A \cup B$ iff $x \in A$ or $x \in B$
					\4	$A \cup B = B \cup A$
					\4	$(A \cup B) \cup C = A \cup (B \cup C)$
					\4	if $A \subset B \rightarrow A \cup B = B$
					\4	$A \cup A = A$
					\4	$A \cup \emptyset = A$
					\4	$A \cup \Omega = \Omega$
				\3	Intersection
					\4	$x \in A \cap B$ iff $x \in A$ and $x \in B$
					\4	Commutative and Associative
					\4	$A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$
					\4	$A \subset B \rightarrow A \cap B = A$
					\4	$A \cap \emptyset = \emptyset$
					\4	$A \cap \Omega = A$
			\2	Mutual Exclusion: $A \cap B = \emptyset$
			\2	Independence:	$P[A \cap B] = P[A]P[B]$ DIFFERENT FROM MUTUAL EXCLUSION
			\2	De-Morgan's Law
				\3	$\bar{A \cup B} = \bar{A} \cap \bar{B}$
				\3	$\bar{A \cap B} = \bar{A} \cup \bar{B}$
			\2	Inclusion-Exclusion
				\3	Size of a union of sets can be written as:
					\4	Sum of Individual sets
					\4	Minus sum over all pairs of sizes of intersection
					\4	plus the sum over all triples of the sizes of their intersections
			\2	Probability Results
				\3	$P[A] = P[A \cup \emptyset] \rightarrow P[\emptyset] = 0$
				\3	$P[A] = 1 - P[\bar{A}] \le 1$
				\3	$A \subset B \rightarrow P[A] \le P[B]$
				\3	$P[A \cup B] = P[A] + P[B] - P[A \cap B]$
		\1	Conditional Probability
			\2	$P[A | B] = \frac{P[A \cap B]}{P[B]}$
		\1	Baye's Rule
			\2	$P[A | B] = \frac{P[B|A] P[A]}{P[B]}$
				\3	Technically $P[A | B] = \frac{P[B | A] = P[A]}{\sum_i P[B|A_i]P[A_i]}$
			\2	$P[A |B]$ is the posterior (knowledge after we know B)
			\2	Conditioning on independent events does not help the probability
		\1	Combinatorics
			\2	Ordered subset or Sampling WITHOUT Replacement
				\3	Permutations	
				\3	Size $n$, with $r$ subsets
				\3	$P = \frac{n!}{(n - r)!}$
			\2	Unordered Subset 
				\3	Combinations
				\3	$C = \binom{n}{r} = \frac{n!}{(n-r)!r!}$
				\3	$r$ heads in $n$ coin tosses
				\3	$\sum_{r = 0}^n \binom{n}{r} = 2^n$
			\2	Birthday Problem
				\3	Ratio of sampling with replacement to sampling without replacement
				\3	$P[Shared] = 1 - \frac{\Pi_{k = 1}^r (365 - k + 1)}{365^r}$
			\2	Occupancy Problem
				\3	Number of distributions of $r$ balls in $n$ cells
				\3	Balls are distinguishable
					\4	$P = n^r$
				\3	Balls are not distinguishable
					\4	$P = \binom{n+r-1}{r}$
		\1	Borel Cantelli Theorem
			\2	$P[A]$ is the probability of infinite number of successes
			\2	$\sum_{n=1}^{\infty} P_n < \infty \rightarrow P[A] = 0$
			\2	$\sum_{n=1}^{\infty} P_n = \infty \rightarrow P[A] = 1$
		\1	Random Variables
			\2	$X : \Omega \rightarrow \mathbb{R}$
			\2	$P[X = x] := P[s : X(s) = x]$
			\2	$P[X \le x] := P[s : X(s) \le x]$
			\2	Probability Mass Function (PMF)
				\3	$P[x] \ge 0~\forall~x$
				\3	$\sum_{x \in \Omega_X}P[x] = 1$
				\3	$B \subset \Omega_X \rightarrow P[B] = \sum_{x \in B}P[x]$
		\1	Types of Random Variables
			\2	Bernoulli RV
				\3	$P[x] = \begin{cases} 1 - p & x = 0 \\ p & x = 1   \end{cases}$
				\3	Coin flip, bit transmit, success and failure
			\2	Geometric RV
				\3	$P[x] = p(1-p)^{x-1}$
				\3	Number of Bernoulli trials until and including the first success
				\3	Toss a coin, probabilty of $x$ tosses until the first heads
			\2	Binomial RV
				\3	$P[x] = \binom{n}{x}p^x(1-p)^{n-x}$
				\3	Flip a coin $n$ times, probability $x$ is the number of heads
			\2	Negative Binomial
				\3	$P[x] = \binom{x-1}{k-1}p^k(1-p)^{x-k},~x=k,k+1,...$
				\3	number of bernoulli trials until and including the $k$th success
			\2	Discrete Uniform Distribution
				\3	$P[x] = \frac{1}{b - a + 1}$
				\3	Uniformly distributed between $[a,b]$
			\2	Poisson
				\3	$P[x] = \frac{e^{-\lambda}\lambda^x}{x!}$
				\3	$\lambda = np$ is the mean
				\3	Poisson approximates the binomial when the number of trials is large
						and the probability of success is small
		\1	Cumulative Distribution Function (CDF)
			\2	$F_X(x) = \sum_{m < x}p(m)$
				\3	CDF is a sum of pmfs
			\2	$F_X(x) = P[X \le x]$
			\2	$F_x(-\infty) = 0$
			\2	$F_x(\infty) = 0$
			\2	$a \ge b \rightarrow F_X(a) \ge F_X(b)$
			\2	$F_X(b) - F_X(a) = P[a < X < b]$
		\1	Continuous Random Variables
			\2	$P[X = x] = 0$
			\2	Cumlative Distribution Function (CDF for Continuous)
				\3	CDF is non-decreasing
				\3	$P[a < X < b] = F_X(b) - F_X(a)$
			\2	Probability Density Function (PDF => PMF)
				\3 $P[a < X < b] = \int_a^b f_X(m) dm$
				\3	$f_X(x) = \pd{}{x}F_X(x)$
				\3	$F_X(x) = \int_{-\infty}^x f_x(m)dm$
				\3	$\int_{-\infty}^{\infty} f_X(x) dx = 1$
		\1	Mixed Random Variables
			\2	Use $\delta(x)$ functions to simulate discrete random variables in a
					continuous setting
		\1	Conditional CDF and PDF
			\2	$F_X(x | X \in B) = P[X \le x | X \in B]$
			\2	$F_X(x | X \in B) = \frac{\int_{B \cap (X \le x)}f_X(m) dm}{\int_B f_X(m) dm}$
			\2	$f_X(x | B) = \pd{}{x}F_X(x | B)$
			\2	$P[B | X = x] = \frac{f_X(x | B)p[B]}{f_X(x)}$
		\1	Continuous Distributions
			\2	Uniform distribution
				\3	$f_X(x) = \frac{1}{b-a}$
				\3	$F_X(x) = \frac{x - a}{b-a}$
			\2	Exponential Distribution
				\3	$f_X(x) = a~exp(-ax)$
				\3	$F_X(x) = 1 - exp(-ax)$
				\3	$P[X > t + s | X > t] = P[X > s]$
					\4	Memoryless property
			\2	Gaussian Distribution
				\3	$f_X(x) = \frac{1}{\sqrt{2\pi \sigma^2}}exp(\frac{-(x - \mu)^2}{2\sigma^2})$
				\3	Standard Normal:	$f_Z(z) = \frac{1}{\sqrt{2\pi}}exp(\frac{-x^2}{2})$
					\4	$z = \frac{x - \mu}{\sigma}$
					\4	Approximation of Binomial:	$Z = \frac{S_n - np}{\sqrt{np(1-p)}}$
			\2	Gamma Distribution
				\3	$f_X(x) = \frac{\lambda exp(-\lambda x) (\lambda x)^{t-1}}{\Gamma (t)}$
					\4	$\Gamma(t) = (t-1)!$
					\4	When $n = 1$ its a form of teh exponential distribution, called chi-squared distribution
		
		\1	Functions of a Random Variable
			\2	$P[Y = y] = P[g(X) = y]$
			\2	$P[X = g^{-1}(y)]$
			\2	$f_Y(y) = f_X(g^{-1}(y)) |\pd{g^{-1}(y)}{y}|$
			\2	Example
				\3	$Y = e^X \rightarrow g(X) = e^x$
				\3	$X ~ U[0,1]$
				\3	$Y = e^X \rightarrow X = ln(Y)$
				\3	$f_Y(y) = f_X(ln(y)) (\frac{1}{y})~1 \le y \le e$
				\3	$f_X(X) = \frac{1}{b - a} = 1 \rightarrow f_X(ln(y)) = 1$
				\3	$f_Y(y) = \frac{1}{y}~1 \le y \le e$
		
		\1	Simulation
			\2	Given $X$ with distribution $f_X(x)$, want $Y$ with distribution $f_Y(y)$
			\2	Find $Y = g(X)$ 
			\2	$Y = F_Y^{-1}(F_X(x))$
			\2	Example:	Have uniform, want exponential
				\3	$F_Y(y) = 1 - e^{-y} = u$
				\3	$e^{-y} = 1 - u$
				\3	$y = -ln(1 - u)$
				\3	$Y = -ln(1 - U) = -ln(U)$
					\4	$U$ and $1 - U$ are both uniform, and so can be interchanged since they're equivalent
	
		\1	Expectation of a Random Variable
			\2	$E[X] = \int_{-\infty}^{\infty}xf_X(x) dx$
				\3	Center of mass interpretation:	
				$E[X] = \frac{\int_{-\infty}^{\infty} xf_X(x)dx}{\int_{-infty}^{\infty} f_X(x)dx}$
			\2	$E[X] = \sum_{x \in \Omega_X}	xP[x]$
			\2	$E[X | B] = \int_{-\infty}^{\infty}x f_{X | B}(x | B)dx$
			\2	Examples
				\3	Bernoulli: $p$
				\3	Gaussian: $\mu$
				\3	Binomial: $n p$
				\3	Poisson: $\lambda$
				\3	Geometric:	$\frac{1}{p}$

			\2	$E[g(X)] = \int_{-\infty}^{\infty}g(X) f_X(dx)$

		\1	Variance of a random variable
			\2	$E[(X - \mu)^2] = E[X^2] - \mu^2$	
			\2	$\sigma^2 \ge 0$
		
		\1	Moments of a random Variable
			\2	Characteristic Function
				\3	$\phi_X(\omega) = \int_{-\infty}^{\infty} e^{j\omega t} f_X(x) dx$
				\3	$\phi_X(\omega) = \sum_{x = \Omega_X}e^{j\omega t} P[x]$
				\3	Inverse Fourier Transform of $f_X(x)$
			\2	Moment Generating Function (MGF)
				\3	$\phi_X(s) = \int_{-\infty}^{\infty} e^{sx} f_X(x) dx$
				\3	$\phi_X(s) = \sum_{x = \Omega_X} e^{sx}P[x]$
				\3	Inverse Laplace Transform of $f_X(x)$
			\2	Calculate Moments from the MGF
				\3	$E[X^n] = \frac{d^n\phi_X(s)}{ds^n}|_{s = 0}$
				\3	$\phi_X(s) = \sum_{n=0}^{\infty} \frac{\phi_X^n(0)s^n}{n!}$

		\1	Probability Inequalities
			\2	Markov:	$P[X \ge c] \le \frac{E[X]}{c}$
			\2	Chebyshev:	$P[|X - \mu| \ge c|] \le \frac{Var(Y)}{c^2}$
			\2	Chernoff:	$P[X \ge c] \le min\{e^{-sc}\phi_X(s)\}$

		\1	Joint PMF
			\2	$P[X = x, Y = y]$
			\2	$P[B] = \sum_{(x,y) \in B}P[x,y]$
			\2	$P[X = x] = \sum_{y \in \Omega_Y}P[x,y]$
			\2	$P[X = x | Y = y] = \frac{P[X=x,Y=y]}{P[Y = y]}$
		
		\1	Joint CDF
			\2	$F_{X,Y}(x,y) = P[X \le x, Y \le y]$
			\2	$f_{X,Y}(x,y) = \frac{\partial^2 F_{X,Y}(x,y)}{\partial x \partial y}$
			\2	$P[x_1 \le X \le x_2, y_1 \le Y \le y_2] =
				F_{X,Y}(x_2,y_2) - F_{X,Y}(x_1,y_2) - F_X(x_2,y_1) + F_{X,Y}(x_1,y_1)$
			\2	$f_{Y | X}(y,x) = \frac{f_{X,Y}(x,y)}{f_X(x)}$

		\1	Expectations involving functions of RV
			\2	$E[X + Y] = E[X] + E[Y]$
		\1	Covariance
			\2	$Cov(X,Y) = E[XY] - E[X]E[Y] = E[(X - \mu_x)(Y - \mu_y)]$ 
			\2	$Var(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y)$
			\2	Correlation:	$\rho(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$
	\end{outline}
\end{document}


















