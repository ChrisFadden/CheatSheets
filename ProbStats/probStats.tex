\documentclass[14pt]{extarticle}
\usepackage{researchPaper}
\usepackage{outlines}

\title{Probability and Statistics Cheat Sheet}
\begin{document}
	\maketitle
%https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/lecture-notes/
	\begin{outline}	
		\1	Terminology
			\2	Sample Space $\Omega$: Set of all possible outcomes
			\2	Event $A \subset \Omega$:	Subset of the sample space
			\2	Union of sets $A \cup B = A + B$
			\2	Intersection of sets $A \cap B = AB$
			\2	Mutual Exclusion: $A_i \cap A_j = \emptyset$ 
			\2	Complement:	$A \cup \overline{A} = \Omega,~A\cap\overline{A} = \emptyset$
			\2	DeMorgan's Law:		$\overline{A \cup B}	 = \overline{A} \cap \overline{B},~\overline{A \cap B} = \overline{A} \cup \overline{B}$
		\1	Axioms of Probability
			\2	$P(A) \ge 0$
			\2	$P(\Omega) = 1$
			\2	$P(A \cup B) = P(A) + P(B)$
		\1	Bayes' Rule
			\2	$P(A | B) = \frac{P(A \cap B)}{P(B)}$
			\2	$P(A \cap B) = P(A | B) P(B)$
			\2	$P(B) = \frac{P(A \cap B)}{P(A | B)}$
			\2	$P(A_i|B) = \frac{P(A_i)P(B|A_i)}{\sum_j P(A_j)P(B | A_j)}$
		\1	Statistical Independence
			\2	$P(A \cap B) = P(A) \cdot P(B)$
			\2	$P(B | A) = P(B)$
			\2	Conditional Independence:	$P(C | B \cap A) = P(C | A)$
		\1	Counting and Combinatorics
			\2	Ordered Samples with replacement: $n^k$
				\3 Ways to arrange 5 letters = $26^5$
			\2	Ordered Sample without replacement: $nPk = \frac{n!}{(n-k)!}$
				\3	Permutation
				\3	Number of 2-subsets of $\{1,2,3,4\} = \frac{4!}{2!}$
			\2	Unordered Samples with replacement: $\binom{n+k-1}{k}$
				\3	Pick 3 cards from deck, putting them back in = $\binom{52 + 2}{3}$
			\2	Unordered Samples without replacement: $nCk = \binom{n}{k} = \frac{n!}{k!(n-k)!}$
				\3	$n$ choose $k$
				\3	Total number of 5 card hands $\binom{52}{5}$
		\1	Discrete Random Variable
			\2	Mapping $X : \Omega \rightarrow \mathbb{R}$
			\2	Probability Mass Function (PMF) $p_X(x) = P(X = x) = P(\{\omega \in \Omega | X(\omega) = x\})$
				\3	$p_X(x) \ge 0$
				\3	$\sum_x p_X(x) = 1$
			\2	Expectation = Mean = $E[X] = \sum_x x p_X(x)$
				\3	$E[g(X)] \neq g(E[X])$
				\3	$E[\alpha X + \beta Y] = \alpha E[X] + \beta E[Y]$
			\2	Variance = $var(X) = E[(X - E[X])^2]$
				\3	Second Moment = $E[X^2] = \sum_x x^2 p_X(x)$
				\3	$var(X) = \sum_x (x - E[X])^2 p_X(x)$
				\3	$var(X) = E[X^2] - (E[X])^2$
				\3	$var(X) \ge 0$
				\3	$var(\alpha X + c) = \alpha^2 var(X)$
		\1	Multiple Discrete Random Variables
			\2	Joint PMF:	$p_{X,Y}[x_i,y_i] = P[X(s) = x_i~and~Y(s) = y_i]$
			\2	$\sum_i \sum_j p_{X,Y}[x_i,y_j] = 1$
			\2	Marginal PMFs
				\3	Obtain the PMF for individual random variables from the joint PDF
				\3	$p_Y[y_k] = \sum_i p_{X,Y}[x_i,y_k]$
				\3	However, the joint PDF cannot be inferred from the marginal PDFs
			\2	Joint CDF:	$P_{X,Y}(x,y) = P[X \le x, Y \le y]$
				\3	$0 \le P_{X,Y}(x,y) \le 1$
				\3	$P_{X,Y}(-\infty,-\infty) = 0$
				\3	$P_{X,Y}(\infty,\infty) = 1$
				\3	Monotonically increases with either x or y
			\2	Marginal CDFs
				\3	$P_X(x) = P_[X \le x] = P[X \le x, Y \le \infty]$
			\2	Independence with Random Variables
				\3	$p_{X,Y}[x_i,y_j] = p_X[x_i]p_Y[y_j]$
				\3	If the joint PMF factors, then the variables are independent
			\2	Expected Value: $E_{X,Y}[g(X,Y)] = \sum_i \sum_j g(x_i,y_j) p_{X,Y}[x_i,y_j]$
				\3	$E_{X,Y}[aX + bY] = aE_X[X] + bE_Y[Y]$
			\2	Covariance:	$Cov(X,Y) = E_{X,Y}[(X - E_X[X])(Y - E_Y[Y])]$
				\3	$Cov(X,Y) = E_{X,Y}[XY] - E_X[X]E_Y[Y]$
				\3	Independent random variables have $Cov(X,Y) = 0$
			\2	Variance: $Var(X + Y) = var(X) + var(Y) + 2cov(X,Y)$
			\2	Correlation coefficient: $\rho_{X,Y} = \frac{cov(X,Y)}{\sqrt{var(X)var(Y)}}$
				\3	$0 \le |\rho_{X,Y}| \le 1$
			\2	Condition PMF
		\1	Continuous Random Variables
		\1	Multiple Continuous Random Variables
		\1	Bernoulli Process
		\1	Poisson Process
		\1	Markov Chains
		\1	Law of Large Numbers
		\1	Central Limit Theorem
		\1	Wiener Filter
		\1	Kalman Filter
	\end{outline}
\end{document}


